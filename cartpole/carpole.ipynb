{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d183680d-2a2a-4b52-bba2-420189a7c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62416fd-a5d5-4495-8fa7-ed45ee9971ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b24e050-d0f7-4109-8c63-daf334e3bfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change back to 1.0 epsilon\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import gc\n",
    "\n",
    "print(\"Change back to 1.0 epsilon\")\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.tau = 0.5\n",
    "        self.batch_size = 100\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 0.1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.double_dqn = True\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        \n",
    "        # model.add(Dense(128, input_dim = self.state_size, activation='relu'))\n",
    "        # model.add(Dense(64, activation='relu'))\n",
    "        # model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='relu'))\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # if self.epsilon > self.epsilon_min:\n",
    "        #     self.epsilon *= self.epsilon_decay\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def replay(self):\n",
    "        minibatch = self.sample(self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            if not done:\n",
    "                # if self.double_dqn:\n",
    "                    predicted_action = np.argmax(target[0])#self.model.predict(next_state,verbose=0)[0])\n",
    "                    target_q = self.target_model.predict(next_state, verbose=0)[0][predicted_action]\n",
    "                    target[0][action] = reward + self.gamma * target_q\n",
    "                # else:\n",
    "                #     target_q = self.target_model.predict(next_state, verbose=0)[0]\n",
    "                #     target[0][action] = reward + self.gamma * max(target_q)\n",
    "            else:\n",
    "                target[0][action] = reward\n",
    "            self.model.fit(state,target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        gc.collect()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        #TODO: Try training without a target network\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1- self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save(self, file):\n",
    "        self.model.save_weights(file)\n",
    "\n",
    "    def load(self, file):\n",
    "        self.model.load_weights(file)\n",
    "        self.target_model.load_weights(file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc41420-302d-4902-8fb7-d519d8ebd776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                     | 10/1000 [00:04<07:02,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.3, Epsilon: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                                     | 20/1000 [00:23<09:15,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.8, Epsilon: 0.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                    | 30/1000 [00:41<09:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 10.0, Epsilon: 0.09801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▏                                                   | 40/1000 [00:59<08:29,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.3, Epsilon: 0.0970299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▋                                                   | 50/1000 [01:18<11:26,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.8, Epsilon: 0.096059601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███▏                                                  | 60/1000 [01:38<11:08,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 10.0, Epsilon: 0.09509900499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███▊                                                  | 70/1000 [01:57<10:41,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.5, Epsilon: 0.0941480149401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████▎                                                 | 80/1000 [02:16<12:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 10.4, Epsilon: 0.093206534790699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▊                                                 | 90/1000 [02:35<10:45,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 10.0, Epsilon: 0.09227446944279201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▎                                               | 100/1000 [02:57<11:52,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.8, Epsilon: 0.09135172474836409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████▊                                               | 110/1000 [03:17<12:35,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.5, Epsilon: 0.09043820750088044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████▎                                              | 120/1000 [03:36<11:19,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.2, Epsilon: 0.08953382542587164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████▉                                              | 130/1000 [03:54<13:40,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.6, Epsilon: 0.08863848717161292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████▍                                             | 140/1000 [04:13<13:36,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 10.0, Epsilon: 0.08775210229989679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████▉                                             | 150/1000 [04:33<14:28,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.9, Epsilon: 0.08687458127689782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████▍                                            | 160/1000 [04:52<16:43,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.2, Epsilon: 0.08600583546412884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████                                            | 170/1000 [05:10<15:53,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.5, Epsilon: 0.08514577710948755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████▌                                           | 180/1000 [05:29<16:37,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 10.7, Epsilon: 0.08429431933839267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████                                           | 190/1000 [05:48<16:54,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.9, Epsilon: 0.08345137614500873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████▌                                          | 200/1000 [06:09<17:33,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.6, Epsilon: 0.08261686238355864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████▏                                         | 210/1000 [06:29<20:11,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.3, Epsilon: 0.08179069375972306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████▋                                         | 220/1000 [06:48<27:02,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.6, Epsilon: 0.08097278682212583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████▏                                        | 230/1000 [07:08<20:48,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg game: 9.9, Epsilon: 0.08016305895390458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████▌                                        | 236/1000 [07:13<23:23,  1.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m dqn_agent\u001b[38;5;241m.\u001b[39mremember(cur_state_, action, reward, observation, done)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_step \u001b[38;5;241m%\u001b[39m dqn_agent\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     dqn_agent\u001b[38;5;241m.\u001b[39mupdate_target_model()\n\u001b[1;32m     32\u001b[0m cur_state_ \u001b[38;5;241m=\u001b[39m observation\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mDDQNAgent.replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m minibatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, next_state, done \u001b[38;5;129;01min\u001b[39;00m minibatch:\n\u001b[0;32m---> 55\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# if self.double_dqn:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             predicted_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(target[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;66;03m#self.model.predict(next_state,verbose=0)[0])\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2513\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2518\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2519\u001b[0m         )\n\u001b[0;32m-> 2521\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1284\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:396\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:148\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:272\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1189\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1188\u001b[0m   \u001b[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1169\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1173\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:707\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.function only supports singleton tf.Variables created on the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst call. Make sure the tf.Variable is only created once or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated outside tf.function. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/function#creating_tfvariables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    705\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiler_with_scope\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_creator_scope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:602\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope\u001b[0;34m(self, scope)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    600\u001b[0m weak_wrapped_fn \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(wrapped_fn)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiler(\u001b[43mtf_decorator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_decorator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_fn\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/tf_decorator.py:147\u001b[0m, in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(target, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    146\u001b[0m   decorator_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(target, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;66;03m# Copy dict entries from target which are not overridden by decorator_func.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m target\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m decorator_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size_, action_size_ = 4, 2\n",
    "dqn_agent = DDQNAgent(state_size_, action_size_)\n",
    "try:\n",
    "    dqn_agent.load('cartpole.h5')\n",
    "except:\n",
    "    print(\"No previous model detected\")\n",
    "n_episodes = 1000\n",
    "n_steps = 200\n",
    "# capacity = 10000\n",
    "\n",
    "streak_step = 0\n",
    "total_step = 0\n",
    "\n",
    "save_episodes = 10\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    cur_state_, _ = env.reset()\n",
    "    for step in range(n_steps):\n",
    "        streak_step += 1\n",
    "        total_step += 1\n",
    "        cur_state_ = np.reshape(cur_state_, [1, state_size_])\n",
    "        action = dqn_agent.act(cur_state_)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        observation = np.reshape(observation, [1, state_size_])\n",
    "        if done:\n",
    "            reward = -5\n",
    "        dqn_agent.remember(cur_state_, action, reward, observation, done)\n",
    "        if total_step % dqn_agent.batch_size == 0:\n",
    "            dqn_agent.replay()\n",
    "            dqn_agent.update_target_model()\n",
    "        cur_state_ = observation\n",
    "        if done:\n",
    "            break\n",
    "    if (episode + 1) % save_episodes == 0:\n",
    "        dqn_agent.save(\"cartpole.h5\")\n",
    "        print(f\"Avg game: {streak_step / save_episodes}, Epsilon: {dqn_agent.epsilon}\")\n",
    "        streak_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb81c8b-9aa3-4e32-9e28-8496dfb0a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't render in jupyter\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = dqn_agent.act(obs)\n",
    "#     obs, rewards, done, _, _ = env.step(action)\n",
    "#     env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef184e62-53f2-4feb-9ee9-6b91071434d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_features, output_values):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_features, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=output_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = functional.selu(self.fc1(x))\n",
    "        x = functional.selu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8a2117-5114-4994-b7d6-306ca592bc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: score: 13.0 - reward: 11.21826354041696\n",
      "Episode 2: score: 14.0 - reward: 12.34218921512365\n",
      "Episode 3: score: 15.0 - reward: 12.88738244101405\n",
      "Episode 4: score: 11.0 - reward: 9.394015821814536\n",
      "Episode 5: score: 27.0 - reward: 24.708146405220027\n",
      "Episode 6: score: 15.0 - reward: 13.197613238915801\n",
      "Episode 7: score: 14.0 - reward: 12.32804586738348\n",
      "Episode 8: score: 67.0 - reward: 58.684392908215536\n",
      "Episode 9: score: 10.0 - reward: 8.721277001313865\n",
      "Episode 10: score: 27.0 - reward: 24.680841796845204\n",
      "Test Episode 10: test score: 12.0 - test reward: 10.585097850859166\n",
      "New best test reward. Saving model\n",
      "Episode 11: score: 19.0 - reward: 16.828974480926988\n",
      "Episode 12: score: 15.0 - reward: 12.62376758605242\n",
      "Episode 13: score: 62.0 - reward: 53.331599742174156\n",
      "Episode 14: score: 25.0 - reward: 21.99505076408386\n",
      "Episode 15: score: 9.0 - reward: 7.689670917391777\n",
      "Episode 16: score: 44.0 - reward: 39.324059683084485\n",
      "Episode 17: score: 30.0 - reward: 26.901629936695095\n",
      "Episode 18: score: 38.0 - reward: 34.19983309507371\n",
      "Episode 19: score: 18.0 - reward: 15.34531607367098\n",
      "Episode 20: score: 15.0 - reward: 13.506186591088774\n",
      "Test Episode 20: test score: 24.0 - test reward: 22.501881715655333\n",
      "New best test reward. Saving model\n",
      "Episode 21: score: 14.0 - reward: 11.978684708476067\n",
      "Episode 22: score: 49.0 - reward: 46.06513425149022\n",
      "Episode 23: score: 21.0 - reward: 19.27572644650936\n",
      "Episode 24: score: 44.0 - reward: 37.351493236422534\n",
      "Episode 25: score: 49.0 - reward: 45.25599856674672\n",
      "Episode 26: score: 22.0 - reward: 18.828860276937487\n",
      "Episode 27: score: 11.0 - reward: 9.645668905228376\n",
      "Episode 28: score: 15.0 - reward: 12.95187089443207\n",
      "Episode 29: score: 15.0 - reward: 13.594075240939855\n",
      "Episode 30: score: 10.0 - reward: 8.421751075983048\n",
      "Test Episode 30: test score: 10.0 - test reward: 8.70790581330657\n",
      "Episode 31: score: 14.0 - reward: 12.460469754040243\n",
      "Episode 32: score: 18.0 - reward: 16.23326661139727\n",
      "Episode 33: score: 24.0 - reward: 20.455465384945274\n",
      "Episode 34: score: 14.0 - reward: 12.285135522484778\n",
      "Episode 35: score: 8.0 - reward: 6.823435831069945\n",
      "Episode 36: score: 10.0 - reward: 8.741531547904016\n",
      "Episode 37: score: 12.0 - reward: 10.275763982534409\n",
      "Episode 38: score: 18.0 - reward: 16.31035004705191\n",
      "Episode 39: score: 15.0 - reward: 13.312253677845003\n",
      "Episode 40: score: 23.0 - reward: 20.76632859557867\n",
      "Test Episode 40: test score: 9.0 - test reward: 7.490555810928345\n",
      "Episode 41: score: 11.0 - reward: 9.517359754443168\n",
      "Episode 42: score: 11.0 - reward: 9.203101858496664\n",
      "Episode 43: score: 15.0 - reward: 13.49810093194246\n",
      "Episode 44: score: 13.0 - reward: 11.501503984630107\n",
      "Episode 45: score: 22.0 - reward: 18.942678798735148\n",
      "Episode 46: score: 12.0 - reward: 10.386452674865724\n",
      "Episode 47: score: 11.0 - reward: 9.744115788489582\n",
      "Episode 48: score: 13.0 - reward: 11.542929101921619\n",
      "Episode 49: score: 13.0 - reward: 11.389836902916432\n",
      "Episode 50: score: 12.0 - reward: 10.340930659323929\n",
      "Test Episode 50: test score: 9.0 - test reward: 8.034889700636267\n",
      "Episode 51: score: 19.0 - reward: 17.160841050371527\n",
      "Episode 52: score: 21.0 - reward: 18.45913887396455\n",
      "Episode 53: score: 12.0 - reward: 9.947436487674715\n",
      "Episode 54: score: 23.0 - reward: 19.567810738086703\n",
      "Episode 55: score: 20.0 - reward: 18.225868037343027\n",
      "Episode 56: score: 12.0 - reward: 10.297499639168382\n",
      "Episode 57: score: 12.0 - reward: 10.350056606531142\n",
      "Episode 58: score: 18.0 - reward: 16.18929210528731\n",
      "Episode 59: score: 11.0 - reward: 9.708243551850318\n",
      "Episode 60: score: 34.0 - reward: 30.90134913250805\n",
      "Test Episode 60: test score: 210.0 - test reward: 172.83252217173566\n",
      "New best test reward. Saving model\n",
      "Episode 61: score: 11.0 - reward: 9.5769311927259\n",
      "Episode 62: score: 12.0 - reward: 10.424733144044874\n",
      "Episode 63: score: 43.0 - reward: 40.39712407961487\n",
      "Episode 64: score: 87.0 - reward: 70.20659799426794\n",
      "Episode 65: score: 31.0 - reward: 28.546696958877146\n",
      "Episode 66: score: 62.0 - reward: 59.59089192394168\n",
      "Episode 67: score: 47.0 - reward: 42.75779418796302\n",
      "Episode 68: score: 54.0 - reward: 50.49808586090802\n",
      "Episode 69: score: 13.0 - reward: 11.349642741680144\n",
      "Episode 70: score: 16.0 - reward: 14.177644316107035\n",
      "Test Episode 70: test score: 15.0 - test reward: 13.594070475548504\n",
      "Episode 71: score: 41.0 - reward: 38.64174120798706\n",
      "Episode 72: score: 22.0 - reward: 20.65497250780463\n",
      "Episode 73: score: 11.0 - reward: 9.533225746452809\n",
      "Episode 74: score: 30.0 - reward: 27.76611640453339\n",
      "Episode 75: score: 20.0 - reward: 18.551372878253463\n",
      "Episode 76: score: 12.0 - reward: 10.47529338821769\n",
      "Episode 77: score: 11.0 - reward: 9.643884348869324\n",
      "Episode 78: score: 17.0 - reward: 15.378014624118807\n",
      "Episode 79: score: 9.0 - reward: 7.797163462638855\n",
      "Episode 80: score: 18.0 - reward: 16.374652862548828\n",
      "Test Episode 80: test score: 10.0 - test reward: 8.872895337641237\n",
      "Episode 81: score: 16.0 - reward: 14.383606034517289\n",
      "Episode 82: score: 12.0 - reward: 10.591471866145731\n",
      "Episode 83: score: 13.0 - reward: 11.452616308629512\n",
      "Episode 84: score: 25.0 - reward: 22.11391147375107\n",
      "Episode 85: score: 13.0 - reward: 11.330124081671237\n",
      "Episode 86: score: 12.0 - reward: 10.704561783000827\n",
      "Episode 87: score: 12.0 - reward: 10.651531213521956\n",
      "Episode 88: score: 9.0 - reward: 7.969503755867481\n",
      "Episode 89: score: 12.0 - reward: 10.304679682850837\n",
      "Episode 90: score: 12.0 - reward: 10.603026412427425\n",
      "Test Episode 90: test score: 10.0 - test reward: 8.66809819638729\n",
      "Episode 91: score: 11.0 - reward: 9.879075044766068\n",
      "Episode 92: score: 10.0 - reward: 8.759188057482245\n",
      "Episode 93: score: 12.0 - reward: 9.849769806861877\n",
      "Episode 94: score: 10.0 - reward: 8.601111344993114\n",
      "Episode 95: score: 10.0 - reward: 8.421971261501312\n",
      "Episode 96: score: 10.0 - reward: 8.797856616973878\n",
      "Episode 97: score: 11.0 - reward: 9.335004860162735\n",
      "Episode 98: score: 12.0 - reward: 10.519387733936309\n",
      "Episode 99: score: 12.0 - reward: 10.632750368118286\n",
      "Episode 100: score: 12.0 - reward: 10.505575619637968\n",
      "Test Episode 100: test score: 9.0 - test reward: 7.571403616666793\n",
      "best test reward: 172.83252217173566\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "use_cuda = True\n",
    "episode_limit = 100\n",
    "target_update_delay = 2  # update target net every target_update_delay episodes\n",
    "test_delay = 10\n",
    "learning_rate = 1e-4\n",
    "epsilon = 1  # initial epsilon\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.9 / 2.5e3\n",
    "gamma = 0.99\n",
    "memory_len = 10000\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "n_features = len(env.observation_space.high)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "memory = deque(maxlen=memory_len)\n",
    "# each memory entry is in form: (state, action, env_reward, next_state)\n",
    "device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()\n",
    "policy_net = Model(n_features, n_actions).to(device)\n",
    "target_net = Model(n_features, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "def get_states_tensor(sample, states_idx):\n",
    "    sample_len = len(sample)\n",
    "    states_tensor = torch.empty((sample_len, n_features), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "    features_range = range(n_features)\n",
    "    for i in range(sample_len):\n",
    "        for j in features_range:\n",
    "            states_tensor[i, j] = sample[i][states_idx][j].item()\n",
    "\n",
    "    return states_tensor\n",
    "\n",
    "\n",
    "def normalize_state(state):\n",
    "    # pass\n",
    "    state[0] /= 2.5\n",
    "    state[1] /= 2.5\n",
    "    state[2] /= 0.3\n",
    "    state[3] /= 0.3\n",
    "\n",
    "\n",
    "def state_reward(state, env_reward):\n",
    "    # return env_reward\n",
    "    return env_reward - (abs(state[0]) + abs(state[2])) / 2.5\n",
    "\n",
    "\n",
    "def get_action(state, e=min_epsilon):\n",
    "    if random.random() < e:\n",
    "        # explore\n",
    "        action = random.randrange(0, n_actions)\n",
    "    else:\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        action = policy_net(state).argmax().item()\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def fit(model, inputs, labels):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    train_ds = TensorDataset(inputs, labels)\n",
    "    train_dl = DataLoader(train_ds, batch_size=5)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in train_dl:\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return total_loss / len(inputs)\n",
    "\n",
    "\n",
    "def optimize_model(train_batch_size=100):\n",
    "    train_batch_size = min(train_batch_size, len(memory))\n",
    "    train_sample = random.sample(memory, train_batch_size)\n",
    "\n",
    "    state = get_states_tensor(train_sample, 0)\n",
    "    next_state = get_states_tensor(train_sample, 3)\n",
    "\n",
    "    q_estimates = policy_net(state.to(device)).detach()\n",
    "    next_state_q_estimates = target_net(next_state.to(device)).detach()\n",
    "    next_actions = policy_net(next_state.to(device)).argmax(dim=1)\n",
    "\n",
    "    for i in range(len(train_sample)):\n",
    "        next_action = next_actions[i].item()\n",
    "        q_estimates[i][train_sample[i][1]] = (state_reward(next_state[i], train_sample[i][2]) +\n",
    "                                              gamma * next_state_q_estimates[i][next_action].item())\n",
    "\n",
    "    fit(policy_net, state, q_estimates)\n",
    "\n",
    "\n",
    "def train_one_episode():\n",
    "    global epsilon\n",
    "    current_state, _ = env.reset()\n",
    "    normalize_state(current_state)\n",
    "    done = False\n",
    "    score = 0\n",
    "    reward = 0\n",
    "    while not done:\n",
    "        action = get_action(current_state, epsilon)\n",
    "        next_state, env_reward, done, _, _ = env.step(action)\n",
    "        normalize_state(next_state)\n",
    "        memory.append((current_state, action, env_reward, next_state))\n",
    "        current_state = next_state\n",
    "        score += env_reward\n",
    "        reward += state_reward(next_state, env_reward)\n",
    "\n",
    "        optimize_model(100)\n",
    "\n",
    "        epsilon -= epsilon_decay\n",
    "\n",
    "    return score, reward\n",
    "\n",
    "\n",
    "def test():\n",
    "    state, _ = env.reset()\n",
    "    normalize_state(state)\n",
    "    done = False\n",
    "    score = 0\n",
    "    reward = 0\n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "        state, env_reward, done, _, _ = env.step(action)\n",
    "        normalize_state(state)\n",
    "        score += env_reward\n",
    "        reward += state_reward(state, env_reward)\n",
    "\n",
    "    return score, reward\n",
    "\n",
    "\n",
    "def main():\n",
    "    best_test_reward = 0\n",
    "\n",
    "    for i in range(episode_limit):\n",
    "        score, reward = train_one_episode()\n",
    "\n",
    "        print(f'Episode {i + 1}: score: {score} - reward: {reward}')\n",
    "\n",
    "        if i % target_update_delay == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            target_net.eval()\n",
    "\n",
    "        if (i + 1) % test_delay == 0:\n",
    "            test_score, test_reward = test()\n",
    "            print(f'Test Episode {i + 1}: test score: {test_score} - test reward: {test_reward}')\n",
    "            if test_reward > best_test_reward:\n",
    "                print('New best test reward. Saving model')\n",
    "                best_test_reward = test_reward\n",
    "                torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
    "\n",
    "    if episode_limit % test_delay != 0:\n",
    "        test_score, test_reward = test()\n",
    "        print(f'Test Episode {episode_limit}: test score: {test_score} - test reward: {test_reward}')\n",
    "        if test_reward > best_test_reward:\n",
    "            print('New best test reward. Saving model')\n",
    "            best_test_reward = test_reward\n",
    "            torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
    "\n",
    "    print(f'best test reward: {best_test_reward}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b9286-0f88-467a-9bef-7b2840d7ef39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
