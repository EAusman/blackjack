{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6bc41420-302d-4902-8fb7-d519d8ebd776",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "6bc41420-302d-4902-8fb7-d519d8ebd776",
        "outputId": "1cb6858d-6169-4b50-f4a5-406b40033e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7a426cf79c48>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-584411fafabe>\u001b[0m in \u001b[0;36mremember\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmodified_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mmodified_next_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use env.reward for next state as it's not yet experienced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified_next_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_reward' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d183680d-2a2a-4b52-bba2-420189a7c928",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d183680d-2a2a-4b52-bba2-420189a7c928",
        "outputId": "fe362f37-e9ae-49b8-b8eb-5d272114e5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f62416fd-a5d5-4495-8fa7-ed45ee9971ea",
      "metadata": {
        "id": "f62416fd-a5d5-4495-8fa7-ed45ee9971ea"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1b24e050-d0f7-4109-8c63-daf334e3bfc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b24e050-d0f7-4109-8c63-daf334e3bfc0",
        "outputId": "e471dbf6-ed15-413f-ed1b-88ef85fafa0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change back to 1.0 epsilon\n"
          ]
        }
      ],
      "source": [
        "# Agent\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.activations import linear\n",
        "\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import gc\n",
        "\n",
        "print(\"Change back to 1.0 epsilon\")\n",
        "class DDQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.tau = 0.5\n",
        "        self.batch_size = 100\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.1\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "        self.double_dqn = True\n",
        "\n",
        "    def build_model(self):\n",
        "        state_size = 4  # Replace with your actual state size\n",
        "        action_size = 2  # Replace with the number of actions\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(16, input_shape=(state_size,), activation=\"linear\"))\n",
        "        model.add(Dense(action_size, activation=\"linear\"))\n",
        "        model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "\n",
        "        # # model.add(Dense(128, input_dim = self.state_size, activation='relu'))\n",
        "        # # model.add(Dense(64, activation='relu'))\n",
        "        # # model.add(Dense(32, activation='relu'))\n",
        "        # model.add(Dense(self.action_size, activation='relu'))\n",
        "        # model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        # super(Model, self).__init__()\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "      # Reshape state and next state before adding them to memory\n",
        "      state = np.reshape(state, [1, self.state_size])\n",
        "      next_state = np.reshape(next_state, [1, self.state_size])\n",
        "      self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # if self.epsilon > self.epsilon_min:\n",
        "        #     self.epsilon *= self.epsilon_decay\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def replay(self):\n",
        "        minibatch = self.sample(self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            self.update_target_model()\n",
        "            target = self.model.predict(state, verbose=0)\n",
        "            if not done:\n",
        "                # if self.double_dqn:\n",
        "                    predicted_action = np.argmax(target[0])#self.model.predict(next_state,verbose=0)[0])\n",
        "                    target_q = self.target_model.predict(next_state, verbose=0)[0][predicted_action]\n",
        "                    target[0][action] = reward + self.gamma * target_q\n",
        "                # else:\n",
        "                #     target_q = self.target_model.predict(next_state, verbose=0)[0]\n",
        "                #     target[0][action] = reward + self.gamma * max(target_q)\n",
        "            else:\n",
        "                target[0][action] = reward\n",
        "            self.model.fit(state,target, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        gc.collect()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        #TODO: Try training without a target network\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1- self.tau)\n",
        "        self.target_model.set_weights(target_weights)\n",
        "\n",
        "    def save(self, file):\n",
        "        self.model.save_weights(file)\n",
        "\n",
        "    def load(self, file):\n",
        "        self.model.load_weights(file)\n",
        "        self.target_model.load_weights(file)\n",
        "    def update_target_model(self, tau=0.01):\n",
        "      \"\"\"\n",
        "      Updates the target network weights using polyak averaging.\n",
        "\n",
        "      Args:\n",
        "        tau: Mixing factor for polyak averaging (generally between 0.001 and 0.1).\n",
        "      \"\"\"\n",
        "      main_weights = self.model.get_weights()\n",
        "      target_weights = self.target_model.get_weights()\n",
        "      for i in range(len(main_weights)):\n",
        "        target_weights[i] = tau * main_weights[i] + (1 - tau) * target_weights[i]\n",
        "      self.target_model.set_weights(target_weights)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state_size_, action_size_ = 4, 2\n",
        "dqn_agent = DDQNAgent(state_size_, action_size_)\n",
        "try:\n",
        "    dqn_agent.load('cartpole.h5')\n",
        "except:\n",
        "    print(\"No previous model detected\")\n",
        "n_episodes = 1000\n",
        "n_steps = 200\n",
        "# capacity = 10000\n",
        "\n",
        "streak_step = 0\n",
        "total_step = 0\n",
        "\n",
        "save_episodes = 10\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    cur_state_, _ = env.reset()\n",
        "    for step in range(n_steps):\n",
        "        streak_step += 1\n",
        "        total_step += 1\n",
        "        cur_state_ = np.reshape(cur_state_, [1, state_size_])\n",
        "        action = dqn_agent.act(cur_state_)\n",
        "        observation, reward, done, _, _ = env.step(action)\n",
        "        observation = np.reshape(observation, [1, state_size_])\n",
        "        if done:\n",
        "            reward = -5\n",
        "        dqn_agent.remember(cur_state_, action, reward, observation, done)\n",
        "        if total_step % dqn_agent.batch_size == 0:\n",
        "            dqn_agent.replay()\n",
        "            dqn_agent.update_target_model()\n",
        "        cur_state_ = observation\n",
        "        if done:\n",
        "            break\n",
        "    if (episode + 1) % save_episodes == 0:\n",
        "        dqn_agent.save(\"cartpole.h5\")\n",
        "        print(f\"Avg game: {streak_step / save_episodes}, Epsilon: {dqn_agent.epsilon}\")\n",
        "        streak_step = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYO-PjNafljR",
        "outputId": "71e91be3-fed2-40b8-97af-b06b224339a6"
      },
      "id": "mYO-PjNafljR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/1000 [00:08<13:17,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.9, Epsilon: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 20/1000 [00:41<16:53,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.4, Epsilon: 0.099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 30/1000 [01:15<16:52,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.5, Epsilon: 0.09801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 40/1000 [01:48<19:02,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.5, Epsilon: 0.0970299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 50/1000 [02:22<19:23,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.4, Epsilon: 0.096059601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 60/1000 [02:57<21:33,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.09509900499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 70/1000 [03:32<20:56,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.0941480149401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 80/1000 [04:06<19:07,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.3, Epsilon: 0.093206534790699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 90/1000 [04:42<22:22,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.09227446944279201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 100/1000 [05:16<19:32,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.2, Epsilon: 0.09135172474836409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 110/1000 [05:52<18:33,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.3, Epsilon: 0.09043820750088044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 120/1000 [06:27<20:04,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.8, Epsilon: 0.08953382542587164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 130/1000 [07:01<16:59,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.9, Epsilon: 0.08863848717161292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 140/1000 [08:07<1:41:57,  7.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 13.3, Epsilon: 0.08687458127689782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 150/1000 [08:40<1:27:04,  6.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.5, Epsilon: 0.08600583546412884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 160/1000 [09:10<1:19:08,  5.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.08514577710948755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 170/1000 [09:40<1:20:31,  5.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.3, Epsilon: 0.08429431933839267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 180/1000 [10:13<1:21:46,  5.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.08345137614500873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 190/1000 [10:44<1:20:26,  5.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.6, Epsilon: 0.08261686238355864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 200/1000 [11:19<1:27:33,  6.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.2, Epsilon: 0.08179069375972306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 210/1000 [11:51<1:18:36,  5.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.2, Epsilon: 0.08097278682212583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 220/1000 [12:26<1:02:53,  4.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.5, Epsilon: 0.08016305895390458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 230/1000 [12:59<57:48,  4.50s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.9, Epsilon: 0.07936142836436554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 240/1000 [13:29<1:10:31,  5.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.4, Epsilon: 0.07856781408072187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 250/1000 [14:00<1:13:11,  5.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.07778213593991465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 260/1000 [14:32<1:14:11,  6.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.3, Epsilon: 0.0770043145805155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 270/1000 [15:05<1:15:28,  6.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.9, Epsilon: 0.07623427143471034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 280/1000 [15:36<1:10:00,  5.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.2, Epsilon: 0.07547192872036323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 290/1000 [16:09<1:41:37,  8.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.7, Epsilon: 0.0747172094331596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 300/1000 [16:47<1:22:48,  7.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.6, Epsilon: 0.073970037338828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 310/1000 [17:24<1:49:56,  9.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.3, Epsilon: 0.07323033696543972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 320/1000 [17:32<11:34,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.2, Epsilon: 0.07323033696543972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 330/1000 [18:04<11:54,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.3, Epsilon: 0.07249803359578533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 340/1000 [18:35<10:22,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.5, Epsilon: 0.07177305325982747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 350/1000 [19:08<12:42,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.6, Epsilon: 0.0710553227272292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 360/1000 [19:39<12:03,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.07034476949995691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 370/1000 [20:12<16:14,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.4, Epsilon: 0.06964132180495734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 380/1000 [20:44<13:22,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.0, Epsilon: 0.06894490858690777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 390/1000 [21:15<15:51,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.0, Epsilon: 0.06825545950103869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 400/1000 [21:47<17:13,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.7, Epsilon: 0.0675729049060283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 410/1000 [22:18<16:05,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.3, Epsilon: 0.06689717585696801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 420/1000 [22:51<18:11,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.8, Epsilon: 0.06622820409839833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 430/1000 [23:23<13:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.5, Epsilon: 0.06556592205741435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 440/1000 [24:00<13:38,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.9, Epsilon: 0.0649102628368402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 450/1000 [24:37<13:48,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.06426116020847181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 460/1000 [25:10<11:33,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 11.3, Epsilon: 0.06361854860638709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 470/1000 [25:41<10:23,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.6, Epsilon: 0.06298236312032322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 480/1000 [26:14<10:24,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.8, Epsilon: 0.06235253948911999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 490/1000 [26:50<11:09,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.1, Epsilon: 0.06172901409422879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 500/1000 [27:25<12:10,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 11.0, Epsilon: 0.061111723953286505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 510/1000 [30:14<2:31:24, 18.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 40.3, Epsilon: 0.05811664141181096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 520/1000 [32:48<1:00:17,  7.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 49.3, Epsilon: 0.05582661385478637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 530/1000 [36:34<2:06:43, 16.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 57.2, Epsilon: 0.05255964875255621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 540/1000 [39:02<1:21:46, 10.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 37.1, Epsilon: 0.05048858887870697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 550/1000 [40:16<39:08,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 20.0, Epsilon: 0.049483865960020704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 560/1000 [40:56<18:56,  2.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 11.7, Epsilon: 0.0489890273004205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 570/1000 [41:34<17:13,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.9, Epsilon: 0.048499137027416296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 580/1000 [42:11<16:17,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.8, Epsilon: 0.048014145657142134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 590/1000 [42:50<16:42,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.6, Epsilon: 0.04753400420057071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 600/1000 [43:27<19:19,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.0, Epsilon: 0.047058664158565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 610/1000 [44:46<35:07,  5.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 20.5, Epsilon: 0.04612219674180956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 620/1000 [45:24<15:41,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 10.0, Epsilon: 0.04566097477439147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 630/1000 [46:02<15:15,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.6, Epsilon: 0.045204365026647556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 640/1000 [46:40<18:44,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.6, Epsilon: 0.04475232137638108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 650/1000 [47:19<18:04,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.8, Epsilon: 0.044304798162617266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 660/1000 [47:56<17:41,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.4, Epsilon: 0.043861750180991095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 670/1000 [48:37<25:55,  4.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 13.6, Epsilon: 0.04342313267918119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 680/1000 [49:15<09:46,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.7, Epsilon: 0.04298890135238938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 690/1000 [51:08<33:22,  6.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 27.8, Epsilon: 0.04171208799332206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 700/1000 [51:47<15:49,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg game: 9.8, Epsilon: 0.041294967113388835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 705/1000 [51:52<06:59,  1.42s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "efb81c8b-9aa3-4e32-9e28-8496dfb0a66e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "efb81c8b-9aa3-4e32-9e28-8496dfb0a66e",
        "outputId": "97190fec-ab8a-4bca-aaf3-79803355345e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5333b8ba90e8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d86a4642b3d>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=('tf.Tensor(shape=(None,), dtype=float32)', {})\n      • training=False\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "# # Doesn't render in jupyter\n",
        "# obs = env.reset()\n",
        "# done = False\n",
        "# while not done:\n",
        "#     action = dqn_agent.act(obs)\n",
        "#     obs, rewards, done, _, _ = env.step(action)\n",
        "#     env.render()\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef184e62-53f2-4feb-9ee9-6b91071434d9",
      "metadata": {
        "id": "ef184e62-53f2-4feb-9ee9-6b91071434d9"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_features, output_values):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=input_features, out_features=32)\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=32)\n",
        "        self.fc3 = nn.Linear(in_features=32, out_features=output_values)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = functional.selu(self.fc1(x))\n",
        "        x = functional.selu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3a8a2117-5114-4994-b7d6-306ca592bc4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3a8a2117-5114-4994-b7d6-306ca592bc4b",
        "outputId": "a383614c-9ddd-4f08-ce75-5d7630e0669d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: score: 24.0 - reward: 21.92471436560154\n",
            "Episode 2: score: 26.0 - reward: 21.789406988024712\n",
            "Episode 3: score: 12.0 - reward: 10.321745324134827\n",
            "Episode 4: score: 37.0 - reward: 33.57567148283125\n",
            "Episode 5: score: 24.0 - reward: 21.02101508677006\n",
            "Episode 6: score: 25.0 - reward: 22.066822504997248\n",
            "Episode 7: score: 49.0 - reward: 45.11534642204642\n",
            "Episode 8: score: 19.0 - reward: 17.188667308539156\n",
            "Episode 9: score: 20.0 - reward: 18.24884291291237\n",
            "Episode 10: score: 30.0 - reward: 26.157555196434263\n",
            "Test Episode 10: test score: 51.0 - test reward: 43.27161414027214\n",
            "New best test reward. Saving model\n",
            "Episode 11: score: 14.0 - reward: 12.515208195149897\n",
            "Episode 12: score: 21.0 - reward: 19.12148921638727\n",
            "Episode 13: score: 33.0 - reward: 27.46333811879159\n",
            "Episode 14: score: 41.0 - reward: 35.4634566962719\n",
            "Episode 15: score: 27.0 - reward: 24.561720565706494\n",
            "Episode 16: score: 24.0 - reward: 19.944308507442475\n",
            "Episode 17: score: 12.0 - reward: 10.114508500695226\n",
            "Episode 18: score: 66.0 - reward: 60.957450225949295\n",
            "Episode 19: score: 111.0 - reward: 105.84136021733282\n",
            "Episode 20: score: 69.0 - reward: 62.33087743893266\n",
            "Test Episode 20: test score: 457.0 - test reward: 377.590272220969\n",
            "New best test reward. Saving model\n",
            "Episode 21: score: 77.0 - reward: 60.0507224738598\n",
            "Episode 22: score: 26.0 - reward: 23.45847371816635\n",
            "Episode 23: score: 108.0 - reward: 81.92444410547616\n",
            "Episode 24: score: 91.0 - reward: 78.2753456927836\n",
            "Episode 25: score: 26.0 - reward: 22.67152660191059\n",
            "Episode 26: score: 16.0 - reward: 14.019223228096964\n",
            "Episode 27: score: 22.0 - reward: 18.530147963762285\n",
            "Episode 28: score: 115.0 - reward: 93.22045385539533\n",
            "Episode 29: score: 31.0 - reward: 26.537538385391233\n",
            "Episode 30: score: 119.0 - reward: 106.80772498324518\n",
            "Test Episode 30: test score: 251.0 - test reward: 214.04203348234293\n",
            "Episode 31: score: 244.0 - reward: 200.88667349591864\n",
            "Episode 32: score: 226.0 - reward: 189.26951923072326\n",
            "Episode 33: score: 217.0 - reward: 182.6690808817745\n",
            "Episode 34: score: 269.0 - reward: 233.9101364549251\n",
            "Episode 35: score: 244.0 - reward: 207.8182301331312\n",
            "Episode 36: score: 288.0 - reward: 249.53469901941722\n",
            "Episode 37: score: 310.0 - reward: 267.8071675531567\n",
            "Episode 38: score: 234.0 - reward: 195.24128098487847\n",
            "Episode 39: score: 248.0 - reward: 208.54707996398224\n",
            "Episode 40: score: 302.0 - reward: 256.17870260700585\n",
            "Test Episode 40: test score: 277.0 - test reward: 234.9283783778548\n",
            "Episode 41: score: 329.0 - reward: 282.3217186288909\n",
            "Episode 42: score: 298.0 - reward: 255.08972248658526\n",
            "Episode 43: score: 327.0 - reward: 282.35589331388445\n",
            "Episode 44: score: 352.0 - reward: 301.3554216973482\n",
            "Episode 45: score: 423.0 - reward: 371.04303917679994\n",
            "Episode 46: score: 338.0 - reward: 289.3855939634142\n",
            "Episode 47: score: 342.0 - reward: 292.1488064501434\n",
            "Episode 48: score: 331.0 - reward: 284.45699005629893\n",
            "Episode 49: score: 401.0 - reward: 347.5623642985707\n",
            "Episode 50: score: 454.0 - reward: 391.4275291506203\n",
            "Test Episode 50: test score: 560.0 - test reward: 486.3634366495535\n",
            "New best test reward. Saving model\n",
            "Episode 51: score: 546.0 - reward: 475.5106859420428\n",
            "Episode 52: score: 509.0 - reward: 444.1554594073916\n",
            "Episode 53: score: 454.0 - reward: 392.20847591478355\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ce59fdf3252a>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-ce59fdf3252a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Episode {i + 1}: score: {score} - reward: {reward}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ce59fdf3252a>\u001b[0m in \u001b[0;36mtrain_one_episode\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ce59fdf3252a>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(train_batch_size)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         q_estimates[i][train_sample[i][1]] = (state_reward(next_state[i], train_sample[i][2]) +\n\u001b[0m\u001b[1;32m    107\u001b[0m                                               gamma * next_state_q_estimates[i][next_action].item())\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ce59fdf3252a>\u001b[0m in \u001b[0;36mstate_reward\u001b[0;34m(state, env_reward)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstate_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# return env_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0menv_reward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Parameters\n",
        "use_cuda = True\n",
        "episode_limit = 100\n",
        "target_update_delay = 2  # update target net every target_update_delay episodes\n",
        "test_delay = 10\n",
        "learning_rate = 1e-4\n",
        "epsilon = 1  # initial epsilon\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.9 / 2.5e3\n",
        "gamma = 0.99\n",
        "memory_len = 10000\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "n_features = len(env.observation_space.high)\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "memory = deque(maxlen=memory_len)\n",
        "# each memory entry is in form: (state, action, env_reward, next_state)\n",
        "device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()\n",
        "policy_net = Model(n_features, n_actions).to(device)\n",
        "target_net = Model(n_features, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "\n",
        "def get_states_tensor(sample, states_idx):\n",
        "    sample_len = len(sample)\n",
        "    states_tensor = torch.empty((sample_len, n_features), dtype=torch.float32, requires_grad=False)\n",
        "\n",
        "    features_range = range(n_features)\n",
        "    for i in range(sample_len):\n",
        "        for j in features_range:\n",
        "            states_tensor[i, j] = sample[i][states_idx][j].item()\n",
        "\n",
        "    return states_tensor\n",
        "\n",
        "\n",
        "def normalize_state(state):\n",
        "    # pass\n",
        "    state[0] /= 2.5\n",
        "    state[1] /= 2.5\n",
        "    state[2] /= 0.3\n",
        "    state[3] /= 0.3\n",
        "\n",
        "\n",
        "def state_reward(state, env_reward):\n",
        "    # return env_reward\n",
        "    return env_reward - (abs(state[0]) + abs(state[2])) / 2.5\n",
        "\n",
        "\n",
        "def get_action(state, e=min_epsilon):\n",
        "    if random.random() < e:\n",
        "        # explore\n",
        "        action = random.randrange(0, n_actions)\n",
        "    else:\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "        action = policy_net(state).argmax().item()\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def fit(model, inputs, labels):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    train_ds = TensorDataset(inputs, labels)\n",
        "    train_dl = DataLoader(train_ds, batch_size=5)\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in train_dl:\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return total_loss / len(inputs)\n",
        "\n",
        "\n",
        "def optimize_model(train_batch_size=100):\n",
        "    train_batch_size = min(train_batch_size, len(memory))\n",
        "    train_sample = random.sample(memory, train_batch_size)\n",
        "\n",
        "    state = get_states_tensor(train_sample, 0)\n",
        "    next_state = get_states_tensor(train_sample, 3)\n",
        "\n",
        "    q_estimates = policy_net(state.to(device)).detach()\n",
        "    next_state_q_estimates = target_net(next_state.to(device)).detach()\n",
        "    next_actions = policy_net(next_state.to(device)).argmax(dim=1)\n",
        "\n",
        "    for i in range(len(train_sample)):\n",
        "        next_action = next_actions[i].item()\n",
        "        q_estimates[i][train_sample[i][1]] = (state_reward(next_state[i], train_sample[i][2]) +\n",
        "                                              gamma * next_state_q_estimates[i][next_action].item())\n",
        "\n",
        "    fit(policy_net, state, q_estimates)\n",
        "\n",
        "\n",
        "def train_one_episode():\n",
        "    global epsilon\n",
        "    current_state, _ = env.reset()\n",
        "    normalize_state(current_state)\n",
        "    done = False\n",
        "    score = 0\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        action = get_action(current_state, epsilon)\n",
        "        next_state, env_reward, done, _, _ = env.step(action)\n",
        "        normalize_state(next_state)\n",
        "        memory.append((current_state, action, env_reward, next_state))\n",
        "        current_state = next_state\n",
        "        score += env_reward\n",
        "        reward += state_reward(next_state, env_reward)\n",
        "\n",
        "        optimize_model(100)\n",
        "\n",
        "        epsilon -= epsilon_decay\n",
        "\n",
        "    return score, reward\n",
        "\n",
        "\n",
        "def test():\n",
        "    state, _ = env.reset()\n",
        "    normalize_state(state)\n",
        "    done = False\n",
        "    score = 0\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        action = get_action(state)\n",
        "        state, env_reward, done, _, _ = env.step(action)\n",
        "        normalize_state(state)\n",
        "        score += env_reward\n",
        "        reward += state_reward(state, env_reward)\n",
        "\n",
        "    return score, reward\n",
        "\n",
        "\n",
        "def main():\n",
        "    best_test_reward = 0\n",
        "\n",
        "    for i in range(episode_limit):\n",
        "        score, reward = train_one_episode()\n",
        "\n",
        "        print(f'Episode {i + 1}: score: {score} - reward: {reward}')\n",
        "\n",
        "        if i % target_update_delay == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "            target_net.eval()\n",
        "\n",
        "        if (i + 1) % test_delay == 0:\n",
        "            test_score, test_reward = test()\n",
        "            print(f'Test Episode {i + 1}: test score: {test_score} - test reward: {test_reward}')\n",
        "            if test_reward > best_test_reward:\n",
        "                print('New best test reward. Saving model')\n",
        "                best_test_reward = test_reward\n",
        "                torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
        "\n",
        "    if episode_limit % test_delay != 0:\n",
        "        test_score, test_reward = test()\n",
        "        print(f'Test Episode {episode_limit}: test score: {test_score} - test reward: {test_reward}')\n",
        "        if test_reward > best_test_reward:\n",
        "            print('New best test reward. Saving model')\n",
        "            best_test_reward = test_reward\n",
        "            torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
        "\n",
        "    print(f'best test reward: {best_test_reward}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35b9286-0f88-467a-9bef-7b2840d7ef39",
      "metadata": {
        "id": "c35b9286-0f88-467a-9bef-7b2840d7ef39"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}